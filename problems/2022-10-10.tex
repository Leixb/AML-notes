%! TEX root = ../000-main.tex
% 2022-10-10

Now, we know that, using Bayes formula:

\begin{align*}
  P(\omega_i|x) &= \frac{P(x|\omega_i)P(\omega_i)}{\sum_{j=1}^c P(x|\omega_j)P(\omega_j)}, \; i=1 \dots c \\
                & \downarrow  a_i := \ln \left\{ P(x|\omega_i)P(\omega_i) \right\} \\
                &= \frac{\exp(a_i)}{\sum_{j=1}^c exp(a_j)} := f_i(x)
\end{align*}

This is called \emph{normalized exponential} or \emph{softmax}.

Suppose that $P(\omega_i|x) >> P(\omega_j | x), \; i \neq j$
$\Rightarrow P(\omega_i | x) \leq 1, P(\omega_j | x) \geq 0, \; i \neq j$
$\sum_{j=1}^c P(x|\omega_j) = 1$.

Suppose further that we want to use a \emph{linear model}:

\begin{equation*}
  a_i = \omega^T \phi(x) = f_i(x), \; i = 1, \dots, c
\end{equation*}

% TODO: (homework)
% Show that the softmax reduces to the logistic regression when $c=2$.
%
% Hint: do the above with c=2 and simplify

Next we have to calculate the likelihood function (We'll use the log likelihood).

\begin{align*}
  \ell(\omega) := \ln\mathcal{L}(\omega) &= \ln
  \prod_{i=1}^n \prod_{j=1}^c P\left(\omega_j|x^i\right)^{\tilde{y}_{ij}} \\
  &= \sum_{i=1}^n \sum_{j=1}^c \tilde{y}_{ij} \ln P(\omega_j | x^i) \\
  &= \sum_{i=1}^n \sum_{j=1}^c \tilde{y}_{ij} \ln \underbrace{f_j(x^i)}_{\hat{y}_{ij}} \\[1em]
  \tilde{y}_{ij} &:= \begin{cases}
    1 & \text{if } x_i \in \omega_j \\
    0 & \text{otherwise}
  \end{cases} \\
    E &:= -2\ln \mathcal{L} \tag{Error function / cross-entropy}
\end{align*}

We want to minimize the \emph{cross-entropy} $E$. To do so, we have to
calculate it numerically through Newton-Raphson \textrightarrow{} IRLS.

\begin{exercise}{}{} Consider a multiclassiflication problem for
$c$ classes defined by prior class probabilities
$\pi_j = P(\omega_j),\, j=1\dots, c$ and class densities
$p_j(x) = P(x|\omega_j)$. We are given training data
$D = \left\{ (x^i, y^i) \right\}_{i=1}^n$ where $y^i \in \{1, \dots, c\}$.

Obtain an estimation of the priors $\hat\pi_j$.


The solution is the proportion of data points labeled with class $\omega_j$.

\tcblower

\begin{align*}
  \hat\pi_j &= \frac{1}{n} \sum_{i=1}^n \mathds{1}\left\{ y^i = j \right\} \\
  &= \frac{1}{n} \sum_{i=1}^n \tilde{y}_{ij}
\end{align*}

\end{exercise}

\begin{exercise}{}{}
\begin{enumerate}
  \item Find estimators $\hat\pi_j f\sim\pi_j$
  \item How correct are they?
  \item Is there any other better estimator?
\end{enumerate}

\begin{hint}
  Look up statistical estimator statistic (bias, variance, consistency, efficiency)
\end{hint}
\end{exercise}
