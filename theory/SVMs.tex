%! TEX root = ../000-main.tex
% 2022-10-07
\chapter[SVMs]{Support Vector Machines}
\index{SVM}

\section{Linear SVM}
\label{sec:linear-svm}
\index{SVM!linear}

\begin{equation*}
	D = \{ (x^1,\, y^1),\, \dots,\, (x^n,\, y^n) \} \quad  x^i \in \mathds{R}^d \; y^i \in \{-1,\, 1\} \mid i = 1,\, \dots,\, n
\end{equation*}

The sum is a \iemph{binary classification}:
\begin{equation}
	\mathcal{F}_{\text{lin}} = \left\{
	x \to f\left(\omega^Tx + b\right) \mid
	\omega \in \mathds{R}^d,
	\; b \in \mathds{R}
	\right\}
\end{equation}

The equation $\pi : \omega^Tx + b = 0$ defines a hyperplane.
The idea is to predict class $+1$ whenever $\omega^Tx + b \geq 0$ and class $-1$ otherwise.

\begin{definition}{linsep}{}
	A dataset is linearly separable (\iemph{linsep}) if:
	\begin{equation*}
		\exists (\omega,\, \omega_0) \in \mathds{R}^d \setminus \{0\} \times \mathds{R}
		\text{ such that: }
		\forall i=1,\,\dots,\,n\quad y^i(\omega^Tx^i +b) > 0
	\end{equation*}
\end{definition}

We assume from now on that $D$ is \emph{linsep}.

\begin{definition}{geometric margin}{}
	The \iemph{geometric margin} of a linear
	classifier $f \to \omega^Tx + b$ \emph{at a data point $x$} is the
	euclidean distance from $x$ to the hyperplane $\pi$:
	\begin{equation*}
		\gamma(x) = \frac{\lVert\omega^Tx + b\rVert}{\lVert\omega\rVert} \tag{geometric margin at $x$}
	\end{equation*}

	\tcbline

	The \iemph{geometric margin of a linear classifier} on $D$ is the minimum of
	these geometric margins over all data points. That is, it is the distance
	from the closest point to the hyperplane:
	\begin{equation*}
		\gamma = \min_{x \in D} \gamma(x) \tag{geometric margin of a linear classifier}
	\end{equation*}

\end{definition}

\index{SVM!hyperplane}
\begin{figure}[H]
	\begin{tikzpicture}
		\begin{axis}[
				domain=0:11,
				xmin=0, xmax=10,
				ymin=0, ymax=10,
				samples=100,
				% axis y line=center,
				% axis x line=bottom,
				xlabel = {$n$},
				legend pos=south east,
				% area style,
				% ticks=none,
			]
			\addplot+[mark=none] {-x + 5};
			\addplot+[mark=none] {x};
			\addlegendentry{\(\pi: \omega^Tx + b = 0\)};
			\addlegendentry{\omega};
		\end{axis}

	\end{tikzpicture}
	\caption{Hyperplane and its normal vector}
\end{figure}

We set into the problem of finding the hyperplane $f$ that minimizes
its geometric margin over the data, $\gamma_{\text{min}}(f)$.

\begin{equation*}
	\max_{(\omega,\, b)}
	\left\{
	\min_{i=1,\,\dots,\, n}
	\frac{| \omega^Tx^i + b |}{\lVert\omega\rVert}
	\right\}
\end{equation*}

\subsection*{Questions}

\begin{enumerate}
	\item Why should we be doing this? ???
	\item Does a solution always exists? Yes
	\item Is the solution unique? Yes
	\item How do we find the solution?

	      The problem boils down to:
	      \begin{align*}
		      \max_{(\omega, b)}
		      \left\{
		      \min_{\substack{i=1,\dots, n                                                  \\ y^i(\omega^Tx^i + b) > 0}}
		      \underbrace{\frac{1}{\lVert\omega\rVert}}_{\text{margin of the solution}}
		      \right\}                                                                      \\
		      \min_{(\omega, b)} \frac{1}{2}\lVert\omega\rVert^2                            \\
		      \text{subject to} \quad y^i(\omega^Tx^i + b) \geq 1 \quad \forall i=1,\dots,n \\
		      \begin{rcases*}
			      \text{convex} \\
			      \text{QP}     \\
		      \end{rcases*} \Rightarrow \text{Solution is unique}
	      \end{align*}

	      \begin{enumerate}
		      \item Convex
		      \item Quadratic Programming (QP)
	      \end{enumerate}

\end{enumerate}

\subsubsection{The dual problem}
\index{SVM!dual}
\index{SVM!lagrangian function}
\index{SVM!lagrangian multiplier}
\index{SVM!KKT}

The Lagrangian function:
\begin{align*}
	\mathscr{L}(\omega, b, \alpha)                   & = \frac{1}{2}\lVert\omega\rVert^2 - \sum_{i=1}^n \alpha_i \left[ y^i(\omega^Tx^i + b) - 1 \right] \\
	\alpha                                           & = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix}
	\tag{Lagrangian multiplier}                                                                                                                          \\
	\frac{\partial \mathscr{L}}{\partial \omega} = 0 & \Rightarrow \omega = \sum_{i=1}^n \alpha_i y^i x^i \tag{p1}\label{eq:lagp1}                       \\
	\frac{\partial \mathscr{L}}{\partial b} = 0      & \Rightarrow \sum_{i=1}^n \alpha_i y^i = 0 \tag{p2}\label{eq:lagp2}                                \\
	\text{KKT} \begin{pmatrix}
		           \text{Karush} \\
		           \text{Kuhn}   \\
		           \text{Tucker}
	           \end{pmatrix} \text{ conditions}                                                                                                          \\
	\forall i=1,\dots,n \quad \alpha_i \left[ y^i(\omega^Tx^i + b) - 1 \right] = 0 \tag{kkt}\label{eq:kkt}
\end{align*}

\begin{enumerate}
	\item \ref{eq:lagp1} \textrightarrow{} The solution $\omega$ must be a linear combination of the TR data points $x^i$
	\item \ref{eq:lagp2} \textrightarrow{} The sum of the Lagrangian multipliers is the same for both classes.
	\item \ref{eq:kkt} \textrightarrow{} $\alpha_i = 0$  xor $y^i(\omega^Tx^i + b) = 1$
	      \begin{enumerate}
		      \item $\alpha_i = 0$ \textrightarrow{} The data point $x^i$ is not a support vector
		      \item $y^i(\omega^Tx^i + b) = 1$ \textrightarrow{} The data point $x^i$ is a support vector. And coincide with the points on the margins of the hyperplanes.
	      \end{enumerate}
\end{enumerate}

% 2022-10-14

\begin{align*}
	\mathscr{L}_\text{dual}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \mathlarger{\sum_{i=1}^n} \sum_{j=1}^n \alpha_i \alpha_j y^i y^j (x^i)^T x^j \\
	\text{goal} \quad \max_{\alpha} \mathscr{L}_\text{dual}(\alpha)                                                                                    \\
	\text{subject to} \quad
	\begin{rcases*}
		\sum_{i=1}^n \alpha_i y^i = 0 \\
		\alpha_i \geq 0
	\end{rcases*} \quad \forall i=1,\dots,n
\end{align*}

\begin{note}{}{}
	The data points $x^i, i=1,\dots,n$  appear \emph{only} as
	dot products.
\end{note}

\begin{note}{}{}
	The (primal) solution vector $\omega$
	can be expressed as a linear combination of the (known) data points $x^i$.
\end{note}

% How \emph{general} are these properties?

\begin{question}{How \emph{general} are these properties?}{}
\end{question}

\begin{question}{What kind of problem do we have?}{}
	Quadratic problem subject to linear (inequality) constraints is
	a \iemph{quadratic programming} (QP) problem.
\end{question}

\index{SVM!primal-dual}
\begin{question}{What is the SVM model?}{}
	We have two versions of the SVM model, the primal and the dual:
	\begin{equation*}
		f_\text{SVM}(x) = \text{sign} \left( \hat{\omega}^T \boldsymbol{x} + \hat{b} \right)
		\quad \hat\omega \in \mathds{R}^d
		\quad b \in \mathds{R}
		\tag{PRIMAL}
	\end{equation*}

	For the \iemph{primal} version, we have $d+1$ parameters to estimate.

	\begin{align*}
		 & = \text{sign} \left( \left(\sum_{i=1}^n \hat\alpha_i y^i x^i \right)^T x + \hat{b} \right) \\
		 & = \text{sign} \left( \sum_{i=1}^n \hat\alpha_i y^i
		\underbrace{(x^i)^T x}_{\text{data}} + \hat{b} \right)
		\tag{DUAL}
	\end{align*}

	For the dual problem, we have $n$ parameters to estimate.
	And the data appears only as dot products.

	\vspace{0.5em}
	We use primal or dual depending on the dataset:
	\begin{enumerate}
		\item If $n \ll d$ we use the primal problem.
		\item If $n \gg d$ we use the dual problem.
	\end{enumerate}
\end{question}

\begin{question}{What kind of model is the dual version of $f_\text{SVM}$?}{}
	\begin{itemize}
		\item Linear (def.~\ref{def:linmod})
		\item Non-Parametric (def.~\ref{def:parametric})
		\item Discriminative (def.~\ref{def:discriminative})
	\end{itemize}
\end{question}

\begin{definition}{Parametric model}{parametric}
	A model is \iemph{parametric} when its size does not
	grow with the size of the training data.
	\begin{enumerate}
		\item size of the model = number of free parameters (unknowns)
		\item size of the training data = $n$
	\end{enumerate}
\end{definition}

\begin{definition}{Leave One Out cross-validation error}{loo}
	\index{LOO}
	$K$-fold cross-validation taken to the extreme: $K=n$.
	Means that we train on $n-1$ data points and test on the remaining one.
	We do this $n$ times, each time leaving out a different data point.
	The LOO error is the average of the $n$ test errors.
\end{definition}

\begin{theorem}{LOO analysis}{looanal}\index{LOO!analysis}

	\begin{lemma}{}{}
		The average LOO-CV error $f_n$ data sets of size $n \geq 2$
		is an unbiased estimator of the average (true) generalization
		error for data sets of size $n=1$.
		\begin{align*}
			\mathds{E}_{D^n \sim p^n}
			\left[ \hat{R}_{LOOCV}(f_{D_n}) \right] =
			\mathds{E}_{D^{n-1} \sim p^{n-1}} \left[{R_{LOOCV}(f_{D_n})} \right]
		\end{align*}
	\end{lemma}
	Let $f_{D_n}$ as in the lemma, let $SV(f_{D_n})$ be the set of
	support vectors in $D_n$. Then:
	\begin{equation*}
		\mathds{E}_{D^{n-1} \sim p^{n-1}} \left[{R(f_{D_n})} \right]
		\leq \mathds{E} \left[
			\frac{\left| SV(f_{D_n}) \right|}{n + 1}
			\right]
	\end{equation*}
	\tcblower
	\begin{note}
		This is an argument in favor of \iemph{sparsity} (def.~\ref{def:sparsity}).
	\end{note}
\end{theorem}

\begin{definition}{Sparsity}{sparsity}\index{sparsity}
	The fraction of data points used in the final model.
\end{definition}

\subsection*{Non-separable data}

This means that there is at least one pair of data point $(x^i, y^i)$
such that $\nexists (\omega, b) \mid y^i (\omega^T x^i + b) \ngeq 1$.

The solution is:

\begin{equation*}
	y^i (\omega^T x^i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0,\quad
	i=1,\dots,n
\end{equation*}

These $\xi_i$ variables are called \iemph{slack variables}.

$\xi_i$ is the amount needed s.t. $y^i (\omega^T x^i + b) \geq 1$.

\begin{figure}[H]
	\begin{tikzpicture}[
			important line/.style={thick,blue,thick},
			dashed line/.style={dashed,blue,thick},
			leftNode/.style={circle,minimum width=.5ex, fill=blue,draw},
			rightNode/.style={rectangle,minimum width=.5ex, fill=red,thick,draw},
		]
		\pgfmathsetseed{1234}
		\begin{axis}[
				width=0.8\textwidth,
				xmin=-50,xmax=50,
				ymin=-50,ymax=50,
				ticks=none,
				axis lines=none,
			]
			\pgfmathsetmacro{\sep}{20};
			\pgfmathsetmacro{\xorig}{120};
			\pgfmathsetmacro{\rot}{60};

			\begin{scope}[rotate around={\rot:(0,0)}]
				\draw[dashed line] (-100, -\sep) -- (100, -\sep);
				\draw[dashed line] (-100, \sep) -- (100, \sep);
				\draw[important line] (-100, 0) -- (100, 0);

				\node[leftNode,label={$x_1$},name=x1] at (-20, \sep) {};
				\node[rightNode,label={$x_2$},name=x2] at (-22, -\sep) {};

				\node[leftNode,label={$x_3$},name=x3] at (-10, -\sep*3/2) {};
				\node[leftNode,label={$x_5$},name=x5] at (-27, -\sep/3) {};

				\node[rightNode,label={$x_4$},name=x4] at (10, -\sep*2/3) {};

				\coordinate (origin) at (0, 0);
				\coordinate (left) at (0, \sep);
				\coordinate (right) at (0, -\sep);

				\coordinate (x3l) at (x3 |- left);
				\coordinate (x4r) at (x4 |- right);
				\coordinate (x5l) at (x5 |- left);

				\begin{scope}[color=purple]
					\draw (x3) -- (x3l) {};
					\draw (x4) -- (x4r) {};
					\draw (x5) -- (x5l) {};
					\node[anchor=north] at ($(x3)!0.5!(x3l)$) {$\xi_3$};
					\node[anchor=north] at ($(x4)!0.5!(x4r)$) {$\xi_4$};
					\node[anchor=north] at ($(x5)!0.5!(x5l)$) {$\xi_5$};
				\end{scope}

				\draw[<->,orange,line width=2pt] (-34,\sep) -- (-34,-\sep);
				\node[orange!90!black,anchor=north,rotate=\rot-90,fill=white] at (-34.25,0) {margin = $\frac{2}{\lVert \omega \rVert}$};

				\pgfplotsinvokeforeach{0.00,0.1,...,1.00}{
					\node [leftNode] at (rand*30-20,\sep+rnd*30) {};
					\node [rightNode] at (rand*30,-\sep-rnd*30) {};
				}

			\end{scope}
			\node[blue,anchor=south east,rotate=\rot] at (2,45) {$\omega^T x + b = 1$};
			\node[blue,anchor=south east,rotate=\rot] at (22,45) {$\pi:\,\omega^T x + b = 0$};
			\node[blue,anchor=south east,rotate=\rot] at (42,45) {$\omega^T x + b = -1$};

		\end{axis}
	\end{tikzpicture}
	\caption{SVM Slack Variables}\label{fig:slack}
\end{figure}

If we remove the points inside the margin, we get a new dataset
which is linearly separable and has 0 training error. This is called
\iemph{soft-margin SVM}, in contrast with a hard-margin SVM, and
the margin will be $\frac{2}{\|\omega\|}$.

Finding the hyperplane with the smallest training error
is NP-hard.

We  have two antagonic goals:
\begin{enumerate}
	\item maximize the margin
	\item \textbf{minimize the training error} An upper bound on the
	      number of training errors is upper bounded by the sum of the slacks:
	      \begin{equation*}
		      n TR \leq \sum_{i=1}^n \xi_i
	      \end{equation*}
\end{enumerate}

The new optimization problem is:
\begin{align*}
	\min_{\omega, b, \xi} &
	\underbrace{\frac{1}{2} \|\omega\|^2}_{\substack{\text{maximize}     \\\text{margin}}}
	+ C \cdot \underbrace{\sum_{i=1}^n \xi_i}_{\substack{\text{minimize} \\\text{TR}\\\text{upper bound of TR}}} \\
	\text{s.t.}           & \begin{rcases*}
		                        y^i (\omega^T x^i + b) \geq 1 - \xi_i \\
		                        \xi_i \geq 0
	                        \end{rcases*}
	\quad i=1,\dots,n
\end{align*}

\index{SVM!trade-off}
We add a hyperparameter $C$ which controls the trade-off between
these two goals.

% 2022-10-21
\subsection{Loss functions for the SVM (classification case)}
\index{SVM!loss functions}

\begin{align*}
	\lVert \xi \rVert_q^q          & = \sum_{i=1}^n \xi_i,\quad q \geq 1      \\
	\lVert \xi \rVert_\infty       & = \max \{ \xi_1,\,\xi_2,\,\dots \xi_n \} \\
	\text{Typical values for $q$:} & \begin{cases*}
		                                 q=1 & \text{1-norm SVM} \\
		                                 q=2 & \text{2-norm SVM} \\
	                                 \end{cases*}
\end{align*}

\begin{figure}[H]
	\begin{tikzpicture}
		\pgfmathsetseed{1234}
		\begin{axis}[
				width=0.8\textwidth,
				height=0.5\textwidth,
				axis equal,
				xmin=-3,xmax=3.5,
				ymin=-1,ymax=5,
				xtick={0,1},
				ytick={0,1},
				axis lines=middle,
				xlabel=$z$,
				yticklabel style={below left},
				legend pos=north east,
				legend cell align={left}
			]
			% quadratic hinge loss
			\draw[green!70!black, line width=2pt] (1,0) -- (2,0);
			\addplot[green!70!black, smooth, line width=2pt, domain=-3:1] {(1 - x)^2};
			\addlegendentry{quadratic hinge loss}

			% hinge loss
			\draw[blue, line width=2pt] (-4,5) -- (1,0) -- (2,0);
			\addlegendimage{draw=blue, line width=2pt};
			\addlegendentry{hinge loss}

			% 0 1 loss
			\draw[red, line width=2pt] (-5,1) -- (0,1);
			\draw[red, line width=2pt](0,0) -- (2,0);
			\draw[red, line width=2pt,dotted](2,0) -- (3,0);
			\addlegendimage{draw=red, line width=2pt};
			\addlegendentry{0-1 loss}


			\node[red, fill=white, draw, circle, inner sep=0pt, minimum size=5pt] at (0,1) {};
			\node[red, fill=red, draw, circle, inner sep=0pt, minimum size=5pt] at (0,0) {};
		\end{axis}
	\end{tikzpicture}
	\caption{Loss functions for the SVM (classification case)}\label{fig:loss}
\end{figure}

\begin{align*}
	L_{0/1} (z)  & = \begin{cases}
		                 0 & \text{if } z \geq 0 \\
		                 1 & \text{if } z < 0
	                 \end{cases} \equiv \mathds{1}_{\{z \leq 0\}}          \\
	L_{hl} (z)   & = \max \{ 0, 1 - z \}                        &  & (q=1) \\
	L_{hl^2} (z) & = \max \{ 0, 1 - z \}^2                      &  & (q=2) \\
\end{align*}

\begin{exercise}{Homework}{}
	repeat all the steps from the primal to the final solution
	(model): exactly the same!

	\begin{equation*}
		0 \leq \alpha_i \leq C
	\end{equation*}
\end{exercise}

\pagebreak
\subsection{SVMs for regression}
\index{SVMR}
\index{SVM!regression}
\begin{marker}
	\paragraph*{Idea}: we want a regression algorithm that maintains the defining
	(and good) features of the SVMC\footnote{Support Vector Machine for Classification}:
\end{marker}
\begin{itemize}
	\item \textbf{Sparsity} and obtention of SVs (Support Vectors).
	\item \textbf{Notion of margin} as a regularizer.
	\item \textbf{Unique solution} (QP)
	\item Data points $x_i$ appear in the model ($f_{SVM}$) and the QP problem
	      \emph{only} as linear products.
	\item Linear models
\end{itemize}

\begin{figure}[H]
	\begin{tikzpicture}[
			important line/.style={thick,blue,thick},
			dashed line/.style={dashed,blue,thick},
			leftNode/.style={circle,minimum width=.5ex, fill=blue,draw},
			rightNode/.style={rectangle,minimum width=.5ex, fill=red,thick,draw},
		]
		\pgfmathsetseed{1234}
		\begin{axis}[
				width=0.8\textwidth,
				xmin=-50,xmax=50,
				ymin=-40,ymax=40,
				ticks=none,
				axis lines=none,
				legend pos=north west,
			]
			\pgfmathsetmacro{\rot}{45};

			\begin{scope}[rotate around={\rot:(0,0)}]

				\draw[fill=blue,opacity=0.4] (-80,-10) rectangle (80,10);

				\draw[line width=2pt] (-80,0) -- (80,0);
				\draw[dashed, line width=2pt] (-80,10) -- (80,10);
				\draw[dashed, line width=2pt] (-80,-10) -- (80,-10);

				\pgfplotsinvokeforeach{1,...,30}{
					\node [rightNode] at (rand*50, rand*15) {};
				};
				\pgfplotsinvokeforeach{1,...,5}{
					\node [rightNode] at (rand*50, rnd*30 + 10) {};
					\node [rightNode] at (rand*50, -rnd*30 - 10) {};
				};

				% \addplot[only marks,red,mark=*,mark size=1pt,domain=-50:50] {rand*30};

				\addlegendimage{legend image code/.code={\node [rightNode,anchor=center] at (2mm,0){};}};
				\addlegendentry{data};
				\addlegendimage{area legend,fill=blue,opacity=0.4};
				\addlegendentry{$\varepsilon$-tube};

			\end{scope}

		\end{axis}
	\end{tikzpicture}
	\caption{SVMR: regression with an $\varepsilon$-tube}
\end{figure}

\begin{itemize}
	\item $\varepsilon > 0$
	\item All points strictly inside the $\epsilon$-tube entail 0 error.
	\item All points (strictly) outside the $\epsilon$-tube entail a positive,
	      equal to the vertical distance to the nearest margin side.
\end{itemize}

\begin{equation*}
	L_{\epsilon} (y,\,y') = \begin{cases}
		0                      & \text{if } |y - y'| \leq \varepsilon \\
		|y - y'| - \varepsilon & \text{otherwise }
	\end{cases}
\end{equation*}

We need to introduce two sets of slacks,
$\overline{\xi_i},\, \underline{\xi_i} \geq 0,\; i=1,\dots,n$,
\index{SVM!slacks}

\begin{align*}
	\min_{w,\,b,\,\xi,\,\overline{\xi},\,\underline{\xi}} &
	\left\{ \frac{1}{2} \|w\|^2 +
	C \sum_{i=1}^n \left(\overline{\xi}_i + \underline{\xi}_i \right) \right\}                                                                          \\
	\text{s.t.}                                           & \quad \begin{rcases*}
		                                                              (\omega^T x_i + b) \geq y_i + \varepsilon + \overline{\xi}_i  \\
		                                                              (\omega^T x_i + b) \leq y_i - \varepsilon - \underline{\xi}_i \\
		                                                              \overline{\xi}_i,\, \underline{\xi}_i \geq 0
	                                                              \end{rcases*} \quad i=1,\dots,n                         \\
	                                                      &                                                                  & \text{Convex QP problem}
\end{align*}

\begin{align*}
	f_{SVMR} (x) & = \sum_{i=1}^n (\overline{\alpha}_i - \underline{\alpha}_i)
	(x^i)^T x + b =                                                                                                                           \\
	             & \downarrow \beta_i = \overline{\alpha}_i - \underline{\alpha}_i                                                            \\
	             & = \sum_{i=1}^n (\beta_i) (x^i)^T x + b                                                                                     \\
	             &                                                                 & \text{Many of }\beta_i\text{ are zero}                   \\
	             &                                                                 & \text{SVs are those }x^i/\beta_i \neq 0                  \\
	             &                                                                 & \text{We have two hyper-parameters: }C,\,\varepsilon > 0 \\
	             &                                                                 & \text{overfitting } C \to \infty,\, \varepsilon \to 0
\end{align*}

